{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Quick, Draw! Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data or download the dataset from https://console.cloud.google.com/storage/browser/quickdraw_dataset/?pli=1.  \n",
    "\n",
    "The architecture was ported across from the tutorial <a href='https://www.tensorflow.org/versions/master/tutorials/recurrent_quickdraw'>Recurrent Neural Networks for Drawing Classification</a> (associated repo available <a href='https://github.com/tensorflow/models/tree/master/tutorials/rnn/quickdraw'>here</a>); of which many of the details have been used here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/googlecreativelab/quickdraw-dataset/raw/master/preview.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras \n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(x, y, batch_size=8):\n",
    "    return x.reshape(batch_size, -1, 3), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_stroke_sequence(x, max_len=80):\n",
    "    padded_x = np.zeros((x.shape[0], max_len, 3), dtype=np.float32)\n",
    "    for i in range(x.shape[0]):\n",
    "        X = x[i]\n",
    "        if X.shape[0] > max_len:\n",
    "            X = X[:max_len, :]\n",
    "        elif X.shape[0] < max_len:\n",
    "            padding = np.array([[0,0,0]] * (max_len-X.shape[0]), dtype=np.float32)            \n",
    "            X = np.vstack((padding, X))\n",
    "            \n",
    "        padded_x[i] = X\n",
    "        \n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape=(80, 3), num_conv=[48, 64, 96], conv_len=[5, 5, 3], dropout=0.3, batch_size=8, \n",
    "                 num_rnn_layers=3, num_rnn_nodes=128, num_classes=174):\n",
    "    \n",
    "    model = models.Sequential() \n",
    "    for i, filters in enumerate(num_conv):\n",
    "        if i == 0:\n",
    "            # TODO: feasible to use a TimeDistributed wrapper here? https://keras.io/layers/wrappers/\n",
    "            model.add(\n",
    "                layers.Conv1D(filters=filters, \n",
    "                              kernel_size=conv_len[i], \n",
    "                              activation=None, \n",
    "                              strides=1, \n",
    "                              padding='same', \n",
    "                              name='conv1d_{}'.format(i), input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(layers.Dropout(dropout, name=\"dropout_{}\".format(i)))\n",
    "            model.add(layers.Conv1D(filters=filters, \n",
    "                                    kernel_size=conv_len[i], \n",
    "                                    activation=None, \n",
    "                                    strides=1, \n",
    "                                    padding='same', \n",
    "                                    name='conv1d_{}'.format(i)))\n",
    "      \n",
    "    for i in range(num_rnn_layers):\n",
    "        model.add(layers.Bidirectional(layers.LSTM(units=num_rnn_nodes, \n",
    "                                                   return_sequences=True, \n",
    "                                                   recurrent_dropout=dropout), \n",
    "                                       name=\"lstm_{}\".format(i)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "    \n",
    "                      \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, batch_size, epochs, train_x, train_y, valid_x, valid_y, max_seq_len=80, \n",
    "          load_previous_weights=True, \n",
    "          model_weights_file=\"output/quickdraw_weights.h5\"):\n",
    "    \n",
    "    # load previous weights (if applicable)\n",
    "    if model_weights_file is not None and os.path.isfile(model_weights_file) and load_previous_weights:\n",
    "        print(\"Loading weights from file {}\".format(model_weights_file))\n",
    "        model.load_weights(model_weights_file)\n",
    "    \n",
    "    # compile model \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    # prepare training and validation data \n",
    "    train_x = pad_stroke_sequence(train_x)\n",
    "    valid_x = pad_stroke_sequence(valid_x)\n",
    "    \n",
    "    checkpoint = callbacks.ModelCheckpoint(model_weights_file, \n",
    "                                           monitor='val_loss', \n",
    "                                           verbose=0, \n",
    "                                           save_best_only=True, \n",
    "                                           save_weights_only=True, \n",
    "                                           mode='auto', \n",
    "                                           period=2)\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    \n",
    "    history = model.fit(train_x, train_y,\n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=(valid_x, valid_y), \n",
    "                        shuffle=True, \n",
    "                        callbacks=[checkpoint, early_stopping])\n",
    "    \n",
    "    return model, history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEST_DIR = '/Volumes/Storage/quickdraw_dataset (subset)/sketchrnn_training_data/'\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 8 \n",
    "MAX_SEQ_LEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (11000,), train_y (11000, 11)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.load(os.path.join(DEST_DIR, \"train_x.npy\"))\n",
    "train_y = np.load(os.path.join(DEST_DIR, \"train_y.npy\"))\n",
    "\n",
    "print(\"train_x {}, train_y {}\".format(train_x.shape, train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (110,), train_y (110, 11)\n"
     ]
    }
   ],
   "source": [
    "valid_x = np.load(os.path.join(DEST_DIR, \"validation_x.npy\"))\n",
    "valid_y = np.load(os.path.join(DEST_DIR, \"validation_y.npy\"))\n",
    "\n",
    "print(\"train_x {}, train_y {}\".format(valid_x.shape, valid_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_0 (Conv1D)            (None, 80, 48)            768       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 80, 64)            15424     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 80, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 80, 96)            18528     \n",
      "_________________________________________________________________\n",
      "lstm_0 (Bidirectional)       (None, 80, 256)           230400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (Bidirectional)       (None, 80, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_2 (Bidirectional)       (None, 80, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 20480)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 11)                225291    \n",
      "=================================================================\n",
      "Total params: 1,278,891\n",
      "Trainable params: 1,278,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(input_shape=(MAX_SEQ_LEN, 3), num_classes=11)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000,)\n",
      "(11000, 80, 3)\n",
      "(110,)\n",
      "(110, 80, 3)\n",
      "Train on 11000 samples, validate on 110 samples\n",
      "Epoch 1/100\n",
      "11000/11000 [==============================] - 1034s - loss: 1.2014 - acc: 0.5827 - val_loss: 0.6307 - val_acc: 0.7727\n",
      "Epoch 2/100\n",
      "11000/11000 [==============================] - 1008s - loss: 0.6778 - acc: 0.7849 - val_loss: 0.4798 - val_acc: 0.8364\n",
      "Epoch 3/100\n",
      "11000/11000 [==============================] - 1011s - loss: 0.4938 - acc: 0.8490 - val_loss: 0.6045 - val_acc: 0.8727\n",
      "Epoch 4/100\n",
      "11000/11000 [==============================] - 1005s - loss: 0.4269 - acc: 0.8754 - val_loss: 0.5494 - val_acc: 0.8818\n",
      "Epoch 5/100\n",
      "11000/11000 [==============================] - 1014s - loss: 0.3846 - acc: 0.8849 - val_loss: 0.5547 - val_acc: 0.9091\n",
      "Epoch 6/100\n",
      "11000/11000 [==============================] - 1000s - loss: 0.3657 - acc: 0.8913 - val_loss: 0.6879 - val_acc: 0.8364\n",
      "Epoch 7/100\n",
      "11000/11000 [==============================] - 979s - loss: 0.3490 - acc: 0.8980 - val_loss: 0.6741 - val_acc: 0.8636\n",
      "Epoch 8/100\n",
      "11000/11000 [==============================] - 976s - loss: 0.3326 - acc: 0.9039 - val_loss: 0.6353 - val_acc: 0.8545\n",
      "Epoch 9/100\n",
      "11000/11000 [==============================] - 979s - loss: 0.3327 - acc: 0.9076 - val_loss: 0.5596 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "11000/11000 [==============================] - 981s - loss: 0.3195 - acc: 0.9107 - val_loss: 0.4043 - val_acc: 0.9091\n",
      "Epoch 11/100\n",
      "11000/11000 [==============================] - 978s - loss: 0.3077 - acc: 0.9128 - val_loss: 0.5294 - val_acc: 0.8727\n",
      "Epoch 12/100\n",
      "11000/11000 [==============================] - 978s - loss: 0.3102 - acc: 0.9184 - val_loss: 0.3714 - val_acc: 0.9091\n",
      "Epoch 13/100\n",
      "11000/11000 [==============================] - 978s - loss: 0.3049 - acc: 0.9211 - val_loss: 0.3445 - val_acc: 0.9091\n",
      "Epoch 14/100\n",
      "11000/11000 [==============================] - 979s - loss: 0.2961 - acc: 0.9157 - val_loss: 0.5555 - val_acc: 0.8909\n",
      "Epoch 15/100\n",
      "11000/11000 [==============================] - 978s - loss: 0.2989 - acc: 0.9203 - val_loss: 0.4983 - val_acc: 0.9091\n",
      "Epoch 16/100\n",
      "11000/11000 [==============================] - 983s - loss: 0.2742 - acc: 0.9235 - val_loss: 0.6739 - val_acc: 0.8909\n",
      "Epoch 17/100\n",
      " 6312/11000 [================>.............] - ETA: 418s - loss: 0.2809 - acc: 0.9276"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-25673830ed9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                 \u001b[0mtrain_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mvalid_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                 max_seq_len=80)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-43973e69a275>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, epochs, train_x, train_y, valid_x, valid_y, max_seq_len)\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         shuffle=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnewnham/anaconda/envs/trondheimdc/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, training_history = train(model, batch_size=BATCH_SIZE, \n",
    "                                epochs=EPOCHS, \n",
    "                                train_x=train_x, train_y=train_y, \n",
    "                                valid_x=valid_x, valid_y=valid_y, \n",
    "                                max_seq_len=MAX_SEQ_LEN, \n",
    "                                load_previous_weights=True, \n",
    "                                model_weights_file=\"output/quickdraw_weights_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy_loss(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
