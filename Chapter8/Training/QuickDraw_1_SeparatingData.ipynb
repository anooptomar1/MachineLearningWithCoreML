{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Quick, Draw! Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data or download the dataset from https://console.cloud.google.com/storage/browser/quickdraw_dataset/?pli=1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/googlecreativelab/quickdraw-dataset/raw/master/preview.jpg'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "from scipy.misc import imresize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASS_FILE = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/sketch_classes.csv'\n",
    "SOURCE_DIR = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/full/simplified/'\n",
    "DEST_DIR = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/sketchrnn_training_data/'\n",
    "STAGING_DIR = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/staging/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data \n",
    "To reduce the size of the data (and demands of training), we will use a subset of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find cateogires similar to that contained within the CLASS_FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_line(ndjson_line):\n",
    "    \"\"\"\n",
    "    Method taken from: \n",
    "    https://www.tensorflow.org/versions/master/tutorials/recurrent_quickdraw\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert string to a JSON object \n",
    "    sample = json.loads(ndjson_line)\n",
    "    label = sample['word']\n",
    "    strokes = sample['drawing']\n",
    "    stroke_lengths = [len(stroke[0]) for stroke in strokes]\n",
    "    total_points = sum(stroke_lengths)\n",
    "    np_strokes = np.zeros((total_points, 3), dtype=np.float32)\n",
    "    current_t = 0 \n",
    "    for stroke in strokes:\n",
    "        for i in [0,1]:\n",
    "            np_strokes[current_t:(current_t + len(stroke[0])), i] = stroke[i]\n",
    "        current_t += len(stroke[0])\n",
    "        np_strokes[current_t - 1, 2] = 1 # stroke end\n",
    "        \n",
    "    # preprocessing \n",
    "    # 1. size normalisation \n",
    "    lower_point = np.min(np_strokes[:, 0:2], axis=0)\n",
    "    upper_point = np.max(np_strokes[:, 0:2], axis=0)\n",
    "    scale = upper_point - lower_point\n",
    "    scale[scale == 0] = 1 \n",
    "    np_strokes[:, 0:2] = (np_strokes[:, 0:2] - lower_point) / scale    \n",
    "    # 2. compute deltas \n",
    "    np_strokes = np.hstack((\n",
    "        np_strokes[1:, 0:2] - np_strokes[0:-1, 0:2], \n",
    "        np_strokes[1:,2].reshape(np_strokes.shape[0]-1, 1)))\n",
    "    \n",
    "    return np_strokes, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(class_filter_file, source_dir, dest_dir, \n",
    "                             num_training_samples=10000, num_validation_samples=1000, \n",
    "                             parts=1, # how many files to distribute the data across \n",
    "                             show_progress_bar=True):\n",
    "    \n",
    "    if show_progress_bar:\n",
    "        from ipywidgets import FloatProgress\n",
    "        from IPython.display import display\n",
    "    \n",
    "        floatProgress = FloatProgress(min=0, max=100)\n",
    "        display(floatProgress)\n",
    "\n",
    "    # load classes     \n",
    "    label_filters = []\n",
    "\n",
    "    with open(class_filter_file, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        for row in csv_reader:\n",
    "            label_filters.append(row[1])\n",
    "\n",
    "    # find matching files \n",
    "    matching_files = [] \n",
    "\n",
    "    for filename in sorted(os.listdir(source_dir)):\n",
    "        full_filepath = os.path.join(source_dir, filename).lower()\n",
    "        if os.path.isfile(full_filepath) and \".ndjson\" in full_filepath.lower():\n",
    "            for label_filter in label_filters:\n",
    "                if label_filter in full_filepath:\n",
    "                    matching_files.append((label_filter, filename))\n",
    "                    break \n",
    "                    \n",
    "    print(\"Found {} matches\".format(len(matching_files)))\n",
    "    \n",
    "    label2idx = {label[0]:idx for idx, label in enumerate(matching_files)}        \n",
    "    \n",
    "    training_stroke_lengths = []\n",
    "    validation_stroke_lengths = []        \n",
    "    \n",
    "    part_num_training_samples = int(num_training_samples / parts)\n",
    "    part_num_validation_samples = int(num_validation_samples / parts)\n",
    "    \n",
    "    print(\"Breaking data into {} parts; each with {} training samples and {} validation samples\".format(\n",
    "        parts, part_num_training_samples, part_num_validation_samples))\n",
    "    \n",
    "    progress_counter = 0\n",
    "    progress_count = len(matching_files) * parts                  \n",
    "    \n",
    "    for part_num in range(parts):                \n",
    "        training_x = []\n",
    "        validation_x = []\n",
    "    \n",
    "        training_y = np.zeros((0,len(matching_files)), dtype=np.int16)\n",
    "        validation_y = np.zeros((0,len(matching_files)), dtype=np.int16)\n",
    "        \n",
    "        line_number = int(part_num * (part_num_training_samples + part_num_validation_samples))\n",
    "        \n",
    "        print(\"Processing part {} of {} - current line number {}\".format(\n",
    "            part_num, parts, line_number))\n",
    "    \n",
    "        for matching_file in matching_files:            \n",
    "            progress_counter += 1\n",
    "            if show_progress_bar:\n",
    "                floatProgress.value = int((float(progress_counter)/float(progress_count)) * 100)\n",
    "            \n",
    "            matching_label = matching_file[0]\n",
    "            matching_filename = matching_file[1]\n",
    "            \n",
    "            with open(os.path.join(source_dir, matching_filename), 'r') as f:\n",
    "                for _ in range(line_number):\n",
    "                    f.readline()                \n",
    "            \n",
    "                for i in range(part_num_training_samples):\n",
    "                    line = f.readline() \n",
    "                    strokes, label = parse_line(line)\n",
    "                    training_stroke_lengths.append(len(strokes))\n",
    "                \n",
    "                    training_x.append(strokes)\n",
    "                \n",
    "                    y = np.zeros(len(matching_files), dtype=np.int16)\n",
    "                    y[label2idx[matching_label]] = 1                                                                                     \n",
    "                    training_y = np.vstack((training_y, y))\n",
    "                \n",
    "                for i in range(part_num_validation_samples):\n",
    "                    line = f.readline() \n",
    "                    strokes, label = parse_line(line)\n",
    "                \n",
    "                    validation_stroke_lengths.append(len(strokes))\n",
    "                    \n",
    "                    validation_x.append(strokes)\n",
    "                \n",
    "                    y = np.zeros(len(matching_files), dtype=np.int16)\n",
    "                    y[label2idx[matching_label]] = 1\n",
    "                    validation_y = np.vstack((validation_y, y))\n",
    "                        \n",
    "        training_x = np.array(training_x) \n",
    "        validation_x = np.array(validation_x)        \n",
    "    \n",
    "        # save .npy            \n",
    "        np.save(os.path.join(dest_dir, \"train_{}_x.npy\".format(part_num)), training_x)\n",
    "        np.save(os.path.join(dest_dir, \"train_{}_y.npy\".format(part_num)), training_y)\n",
    "        \n",
    "        np.save(os.path.join(dest_dir, \"validation_{}_x.npy\".format(part_num)), validation_x)\n",
    "        np.save(os.path.join(dest_dir, \"validation_{}_y.npy\".format(part_num)), validation_y)\n",
    "    \n",
    "    training_stroke_lengths = np.array(training_stroke_lengths)\n",
    "    validation_stroke_lengths = np.array(validation_stroke_lengths)\n",
    "    \n",
    "    np.save(os.path.join(dest_dir, \"train_stroke_lengths.npy\"), training_stroke_lengths)\n",
    "    np.save(os.path.join(dest_dir, \"validation_stroke_lengths.npy\"), validation_stroke_lengths)          \n",
    "        \n",
    "    print(\"Finished\")\n",
    "    \n",
    "    print(\"Training stroke lens: Mean {}, Min {}, Max {}\".format(\n",
    "        np.mean(training_stroke_lengths), \n",
    "        np.min(training_stroke_lengths), \n",
    "        np.max(training_stroke_lengths)))\n",
    "    \n",
    "    print(\"Validation stroke lens: Mean {}, Min {}, Max {}\".format(\n",
    "        np.mean(validation_stroke_lengths), \n",
    "        np.min(validation_stroke_lengths), \n",
    "        np.max(validation_stroke_lengths)))        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_preprocess_data(class_filter_file=CLASS_FILE, \n",
    "                         source_dir=SOURCE_DIR, \n",
    "                         num_training_samples=10000, \n",
    "                         num_validation_samples=1000, \n",
    "                         dest_dir=DEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stroke_lens = np.load(os.path.join(DEST_DIR, \"train_stroke_lengths.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stroke_lens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = [75, 80, 85, 90, 95]\n",
    "\n",
    "lines_percentiles = [np.percentile(train_stroke_lens, p) for p in percentile]\n",
    "\n",
    "plt.bar(range(len(lines_percentiles)), \n",
    "        lines_percentiles, align='center')\n",
    "plt.xticks(range(len(percentile)), [p for p in percentile])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Copy matching files across to a staging folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def migrate_matching_files(class_filter_file, source_dir, dest_dir, \n",
    "                           num_samples=11000, \n",
    "                           show_progress_bar=True):\n",
    "    \n",
    "    if show_progress_bar:\n",
    "        from ipywidgets import FloatProgress\n",
    "        from IPython.display import display\n",
    "    \n",
    "        floatProgress = FloatProgress(min=0, max=100)\n",
    "        display(floatProgress)\n",
    "\n",
    "    # load classes     \n",
    "    label_filters = []\n",
    "\n",
    "    with open(class_filter_file, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        for row in csv_reader:\n",
    "            label_filters.append(row[1])\n",
    "\n",
    "    # find matching files \n",
    "    matching_files = [] \n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        full_filepath = os.path.join(source_dir, filename).lower()\n",
    "        if os.path.isfile(full_filepath) and \".ndjson\" in full_filepath.lower():\n",
    "            for label_filter in label_filters:\n",
    "                if label_filter in full_filepath or label_filter == full_filepath:\n",
    "                    matching_files.append((label_filter, filename))\n",
    "                    break \n",
    "                    \n",
    "    print(\"Found {} matches\".format(len(matching_files)))               \n",
    "        \n",
    "    # build dataset \n",
    "    for matching_file_idx, matching_file in enumerate(matching_files):\n",
    "        if show_progress_bar:\n",
    "            floatProgress.value = int((float(matching_file_idx+1)/float(len(matching_files))) * 100) \n",
    "        \n",
    "        matching_label = matching_file[0]\n",
    "        matching_file = matching_file[1]\n",
    "        \n",
    "        full_filepath = os.path.join(source_dir, matching_file)                                \n",
    "        \n",
    "        json_packet = []\n",
    "        \n",
    "        with open(full_filepath) as f:            \n",
    "            for idx, line in enumerate(f):                                    \n",
    "                sample = json.loads(line)\n",
    "                json_packet.append(\n",
    "                    {\n",
    "                        \"word\": \"'{}'\".format(sample['word']), \n",
    "                        \"drawing\":sample['drawing']\n",
    "                    })\n",
    "                \n",
    "                if len(json_packet) >= num_samples:\n",
    "                    break                 \n",
    "    \n",
    "        with open(os.path.join(dest_dir, matching_file), \"w\") as f:\n",
    "            json.dump(json_packet, f)        \n",
    "        \n",
    "    print(\"Finished\")                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migrate_matching_files(class_filter_file=CLASS_FILE, \n",
    "                       source_dir=SOURCE_DIR, \n",
    "                       num_samples=11000, \n",
    "                       dest_dir=STAGING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
